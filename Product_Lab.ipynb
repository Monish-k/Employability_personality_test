{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed330c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM,Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "np.random.seed(7)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65a12dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "str_punc = string.punctuation\n",
    "\n",
    "engstopwords = stopwords.words(\"english\")\n",
    "engstopwordsV2 = re.sub('[' + re.escape(string.punctuation) + ']', '',\n",
    "                        ' '.join(engstopwords)).split()\n",
    "\n",
    "engstopwords = set(engstopwords).union(set(engstopwordsV2))\n",
    "\n",
    "def lemmatize(word):\n",
    "    word = wnl.lemmatize(word, 'a')\n",
    "    word = wnl.lemmatize(word, 'v')\n",
    "    word = wnl.lemmatize(word, 'n')\n",
    "    return word\n",
    "\n",
    "def clean(text):\n",
    "    # Remove URLs from text\n",
    "    text = re.sub(\"http.*?([ ]|\\|\\|\\||$)\", \"\", text).lower()\n",
    "    url_regex = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    text = re.sub(url_regex, \"\", text)\n",
    "\n",
    "    # Remove specific punctuation (usually associated with a word)\n",
    "    text = re.sub(r'(:|;).', \" \", text)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    text = re.sub('['+re.escape(str_punc)+']',\" \",  text)\n",
    "    \n",
    "    # Remove parantheses, brackets\n",
    "    text = re.sub('(\\[|\\()*\\d+(\\]|\\))*', ' ', text)\n",
    "    \n",
    "    # Remove string marks\n",
    "    text = re.sub('[’‘“\\.”…–]', '', text)\n",
    "    text = re.sub('[^(\\w|\\s)]', '', text)\n",
    "    text = re.sub('(gt|lt)', '', text)\n",
    "    \n",
    "    #Check that each word is not stopword, and lemmatize it\n",
    "    text = list(map(lemmatize, text.split()))\n",
    "    text = [word for word in text if (word not in engstopwords)]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48d0279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8675</td>\n",
       "      <td>8675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>16</td>\n",
       "      <td>8675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'Yeah, I suppose so. I built my current comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                              posts\n",
       "count   8675                                               8675\n",
       "unique    16                                               8675\n",
       "top     INFP  'Yeah, I suppose so. I built my current comput...\n",
       "freq    1832                                                  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"mbti_1.csv\")\n",
    "data.describe(include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00b1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['posts']=data['posts'].apply(lambda x : clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aabc984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>type</th>\n",
       "      <th>Extrovert</th>\n",
       "      <th>Sensitive</th>\n",
       "      <th>Thinking</th>\n",
       "      <th>Judging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfp intj moment sportscenter top ten play pra...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>find lack post alarm sex bore position often e...</td>\n",
       "      <td>ENTP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good one course say know bless curse absolutel...</td>\n",
       "      <td>INTP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear intp enjoy conversation day esoteric gabb...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fire another silly misconception approach logi...</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               posts  type  Extrovert  \\\n",
       "0  enfp intj moment sportscenter top ten play pra...  INFJ          0   \n",
       "1  find lack post alarm sex bore position often e...  ENTP          1   \n",
       "2  good one course say know bless curse absolutel...  INTP          0   \n",
       "3  dear intp enjoy conversation day esoteric gabb...  INTJ          0   \n",
       "4  fire another silly misconception approach logi...  ENTJ          1   \n",
       "\n",
       "   Sensitive  Thinking  Judging  \n",
       "0          0         0        1  \n",
       "1          0         1        0  \n",
       "2          0         1        0  \n",
       "3          0         1        1  \n",
       "4          0         1        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data = data.copy()\n",
    "processed_data = processed_data.reindex(columns=[\"posts\",'type','Extrovert','Sensitive','Thinking','Judging'])\n",
    "\n",
    "for index,value in enumerate(processed_data['type']):\n",
    "    if 'E' in value:\n",
    "        processed_data.iloc[index,2]=1\n",
    "    else:\n",
    "        processed_data.iloc[index,2]=0\n",
    "    if 'S' in value:\n",
    "        processed_data.iloc[index,3]=1\n",
    "    else:\n",
    "        processed_data.iloc[index,3]=0\n",
    "    if 'T' in value:\n",
    "        processed_data.iloc[index,4]=1\n",
    "    else:\n",
    "        processed_data.iloc[index,4]=0\n",
    "    if 'J' in value:\n",
    "        processed_data.iloc[index,5]=1\n",
    "    else:\n",
    "        processed_data.iloc[index,5]=0\n",
    "\n",
    "processed_data = processed_data.astype({\"Extrovert\": int,'Sensitive':int,'Thinking':int,'Judging':int}, errors='ignore')\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e14c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = processed_data['posts'].copy()\n",
    "data_Y = processed_data['type'].copy()\n",
    "data_1 = processed_data['Extrovert'].copy()\n",
    "data_2 = processed_data['Sensitive'].copy()\n",
    "data_3 = processed_data['Thinking'].copy()\n",
    "data_4 = processed_data['Judging'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c066d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for binary classification\n",
    "X_train_1,X_test_1,Y_train_1,Y_test_1 = train_test_split(data_X,data_1,test_size=.2,random_state=1)\n",
    "X_train_2,X_test_2,Y_train_2,Y_test_2 = train_test_split(data_X,data_2,test_size=.2,random_state=1)\n",
    "X_train_3,X_test_3,Y_train_3,Y_test_3 = train_test_split(data_X,data_3,test_size=.2,random_state=1)\n",
    "X_train_4,X_test_4,Y_train_4,Y_test_4 = train_test_split(data_X,data_4,test_size=.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442d379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99635ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# Encode training data sentences into sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train_1)\n",
    "# Get max training sequence length\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "# Pad the training sequences\n",
    "train_padded = pad_sequences(train_sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24306928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode test data sentences into sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test_1)\n",
    "# Pad the training sequences\n",
    "test_padded = pad_sequences(test_sequences, maxlen=maxlen)\n",
    "\n",
    "embedding_vector_length=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0521d260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.asarray(Y_train_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2a49a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 922, 32)           3200000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 922, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 461, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,256,405\n",
      "Trainable params: 3,256,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6940 samples\n",
      "Epoch 1/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.5741 - accuracy: 0.7646WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 137s 20ms/sample - loss: 0.5745 - accuracy: 0.7643\n",
      "Epoch 2/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.5226 - accuracy: 0.7739WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 121s 17ms/sample - loss: 0.5230 - accuracy: 0.7736\n",
      "Epoch 3/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.4853 - accuracy: 0.7736WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 120s 17ms/sample - loss: 0.4851 - accuracy: 0.7736\n",
      "Epoch 4/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.4344 - accuracy: 0.7753WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 119s 17ms/sample - loss: 0.4343 - accuracy: 0.7754\n",
      "Epoch 5/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.3165 - accuracy: 0.8244WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 121s 17ms/sample - loss: 0.3162 - accuracy: 0.8248\n",
      "Epoch 6/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.2144 - accuracy: 0.9146WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 120s 17ms/sample - loss: 0.2142 - accuracy: 0.9147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa0753e80b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(100000, embedding_vector_length, input_length=maxlen))\n",
    "model_1.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model_1.add(MaxPooling1D(pool_size=2))\n",
    "model_1.add(LSTM(100))\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_1.summary())\n",
    "filepath=\"/Users/moni/Code/PE-Product/weights_best_cnn_1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "model_1.fit(train_padded, np.asarray(Y_train_1), epochs=6, batch_size=256,verbose = 1,callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e49f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 922, 32)           3200000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 922, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 461, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,256,405\n",
      "Trainable params: 3,256,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6940 samples\n",
      "Epoch 1/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.4849 - accuracy: 0.8607WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 125s 18ms/sample - loss: 0.4857 - accuracy: 0.8601\n",
      "Epoch 2/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.3993 - accuracy: 0.8598WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 119s 17ms/sample - loss: 0.3989 - accuracy: 0.8601\n",
      "Epoch 3/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.3777 - accuracy: 0.8602WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 124s 18ms/sample - loss: 0.3778 - accuracy: 0.8601\n",
      "Epoch 4/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.3005 - accuracy: 0.8610WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 121s 17ms/sample - loss: 0.2998 - accuracy: 0.8612\n",
      "Epoch 5/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.8890WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 117s 17ms/sample - loss: 0.2018 - accuracy: 0.8893\n",
      "Epoch 6/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9611WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 116s 17ms/sample - loss: 0.1104 - accuracy: 0.9612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9fb4744ba8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(100000, embedding_vector_length, input_length=maxlen))\n",
    "model_2.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model_2.add(MaxPooling1D(pool_size=2))\n",
    "model_2.add(LSTM(100))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_2.summary())\n",
    "filepath=\"/Users/moni/Code/PE-Product/weights_best_cnn_2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "model_2.fit(train_padded, np.asarray(Y_train_2), epochs=6, batch_size=256,verbose = 1,callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfb75400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 922, 32)           3200000   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 922, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 461, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,256,405\n",
      "Trainable params: 3,256,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6940 samples\n",
      "Epoch 1/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5414WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 129s 19ms/sample - loss: 0.6893 - accuracy: 0.5415\n",
      "Epoch 2/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.6721 - accuracy: 0.5781WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 118s 17ms/sample - loss: 0.6721 - accuracy: 0.5784\n",
      "Epoch 3/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.6558 - accuracy: 0.6450WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 118s 17ms/sample - loss: 0.6555 - accuracy: 0.6448\n",
      "Epoch 4/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.4908 - accuracy: 0.7934WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 119s 17ms/sample - loss: 0.4910 - accuracy: 0.7932\n",
      "Epoch 5/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.4369 - accuracy: 0.8226WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 118s 17ms/sample - loss: 0.4368 - accuracy: 0.8228\n",
      "Epoch 6/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.3541 - accuracy: 0.8698WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 118s 17ms/sample - loss: 0.3539 - accuracy: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9f76e64668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(100000, embedding_vector_length, input_length=maxlen))\n",
    "model_3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model_3.add(MaxPooling1D(pool_size=2))\n",
    "model_3.add(LSTM(100))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_3.summary())\n",
    "filepath=\"/Users/moni/Code/PE-Product/weights_best_cnn_3.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "model_3.fit(train_padded, np.asarray(Y_train_3), epochs=6, batch_size=256,verbose = 1,callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "453857e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 922, 32)           3200000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 922, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 461, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,256,405\n",
      "Trainable params: 3,256,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6940 samples\n",
      "Epoch 1/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.6740 - accuracy: 0.5949WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 125s 18ms/sample - loss: 0.6740 - accuracy: 0.5948\n",
      "Epoch 2/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.6593 - accuracy: 0.6030WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 119s 17ms/sample - loss: 0.6594 - accuracy: 0.6027\n",
      "Epoch 3/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.6592 - accuracy: 0.6768WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 123s 18ms/sample - loss: 0.6593 - accuracy: 0.6759\n",
      "Epoch 4/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.4812 - accuracy: 0.7869WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 118s 17ms/sample - loss: 0.4810 - accuracy: 0.7867\n",
      "Epoch 5/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.8834WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 126s 18ms/sample - loss: 0.3184 - accuracy: 0.8833\n",
      "Epoch 6/6\n",
      "6912/6940 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9326WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "6940/6940 [==============================] - 124s 18ms/sample - loss: 0.1836 - accuracy: 0.9326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9f34489ac8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4 = Sequential()\n",
    "model_4.add(Embedding(100000, embedding_vector_length, input_length=maxlen))\n",
    "model_4.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model_4.add(MaxPooling1D(pool_size=2))\n",
    "model_4.add(LSTM(100))\n",
    "model_4.add(Dense(1, activation='sigmoid'))\n",
    "model_4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_4.summary())\n",
    "filepath=\"/Users/moni/Code/PE-Product/weights_best_cnn_4.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "model_4.fit(train_padded, np.asarray(Y_train_4), epochs=6, batch_size=256,verbose = 1,callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee957e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save(\"/Users/moni/code/PE-Product/model_1.h5\")\n",
    "model_2.save(\"/Users/moni/code/PE-Product/model_2.h5\")\n",
    "model_3.save(\"/Users/moni/code/PE-Product/model_3.h5\")\n",
    "model_4.save(\"/Users/moni/code/PE-Product/model_4.h5\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pickle\",\"wb\") as token_handle:\n",
    "    pickle.dump(tokenizer,token_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48680693",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I don't like to be social, I solve problems using highly optimized solutions, and I use my intuition to predict possible scenarios in the future, some people consider me as bossy, but I'm not, recognized me?\"\n",
    "text = clean(text)\n",
    "test = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "## Pad the training sequences\n",
    "#test_padded = pad_sequences(test, maxlen=maxlen)\n",
    "#print(test_padded.shape)\n",
    "#model_1.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5de62f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model = models.load_model(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3eaa30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43012494]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"find lack post alarm sex bore position often!\"\n",
    "text = clean(text)\n",
    "\n",
    "with open(\"tokenizer.pickle\",\"rb\") as token_handle:\n",
    "    token = pickle.load(token_handle)\n",
    "\n",
    "test = token.texts_to_sequences([text])\n",
    "test_padded = pad_sequences(test, maxlen=maxlen)\n",
    "#print(test_padded.shape)\n",
    "model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71cf5858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6676\n",
       "1    1999\n",
       "Name: Extrovert, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b7d46dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7478\n",
       "1    1197\n",
       "Name: Sensitive, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03be8106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4694\n",
       "1    3981\n",
       "Name: Thinking, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ff76377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5241\n",
       "1    3434\n",
       "Name: Judging, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e216730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
